{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 17.8\n",
    "### Homework Practice in Markov Decision Processes\n",
    "##### Value iteration\n",
    "\n",
    "> Consider the 3 × 3 world shown in Figure 17.14(a). The transition model is the same as in the 4 × 3 Figure 17.1: 80% of the time the agent goes in the direction it selects; the rest of the time it moves at right angles to the intended direction.\n",
    "Implement value iteration for this world for each value of r below. Use discounted\n",
    "rewards with a discount factor of 0.99. Show the policy obtained in each case. Explain intuitively why the value of r leads to each policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ThreeByThreeWorld:\n",
    "    reward = {}\n",
    "    U = {}\n",
    "    pi = {}\n",
    "    terminal_s = (0, 2)\n",
    "    ds = [(0,1), (1,0), (0,-1), (-1,0)]\n",
    "    direction = ['r', 'd', 'l', 'u']\n",
    "    \n",
    "    def __init__(self, r):\n",
    "        for y in range(3):\n",
    "            for x in range(3):\n",
    "                s = (y, x)\n",
    "                self.reward[s] = -1\n",
    "        self.reward[(0,0)] = r\n",
    "        self.reward[(0,2)] = 10\n",
    "        \n",
    "    def get_reward(self):\n",
    "        for y in range(3):\n",
    "            for x in range(3):\n",
    "                s = (y, x)\n",
    "                print('{:5.1f}'.format(self.reward[s]), end=' ')\n",
    "            print()\n",
    "        return self.reward\n",
    "    \n",
    "    def get_U(self):\n",
    "        for y in range(3):\n",
    "            for x in range(3):\n",
    "                s = (y, x)\n",
    "                print('{:5.1f}'.format(self.U[s]), end=' ')\n",
    "            print()\n",
    "        return self.U\n",
    "    \n",
    "    def get_pi(self):\n",
    "        for y in range(3):\n",
    "            for x in range(3):\n",
    "                s = (y, x)\n",
    "                if s in self.pi.keys():\n",
    "                    print(self.direction[self.pi[s]], end=' ')\n",
    "                else:\n",
    "                    print('.', end=' ')\n",
    "            print()\n",
    "        return self.pi\n",
    "    \n",
    "    def move_state(self, s, d):\n",
    "        ts = tuple(map(lambda x,y:x+y, s, self.ds[d]))\n",
    "        if ts[0] < 0 or ts[0] >= 3 or ts[1] < 0 or ts[1] >= 3:\n",
    "            ts = s\n",
    "        return ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(self, discount, error, verbose=False):\n",
    "    U_prime = self.reward.copy()\n",
    "    while True:\n",
    "        delta = 0\n",
    "        self.U = U_prime.copy()\n",
    "        for y in range(3):\n",
    "            for x in range(3):\n",
    "                s = (y, x)\n",
    "                if s != self.terminal_s:\n",
    "                    maxExpectU = -1e9\n",
    "                    for act in range(4):\n",
    "                        expectU = 0\n",
    "                        for d in range(4):\n",
    "                            ts = self.move_state(s, d)\n",
    "\n",
    "                            transition_prob = 0\n",
    "                            if d == act:\n",
    "                                transition_prob = 0.8\n",
    "                            elif (d-act+4) % 4 == 1 or (d-act+4) % 4 == 3:\n",
    "                                transition_prob = 0.1\n",
    "\n",
    "                            U_ts = 0\n",
    "                            if ts in self.U.keys():\n",
    "                                U_ts = self.U[ts]\n",
    "\n",
    "                            expectU += transition_prob * U_ts\n",
    "\n",
    "                        if maxExpectU < expectU:\n",
    "                            self.pi[s] = act\n",
    "                            maxExpectU = expectU\n",
    "                    U_prime[s] = self.reward[s] + discount*maxExpectU\n",
    "                else:\n",
    "                    U_prime[s] = self.reward[s]\n",
    "\n",
    "                if verbose:\n",
    "                    print('{:10.1f}'.format(self.U[s]), end='(')\n",
    "                    print('{})'.format(self.pi[s]), end=' ')\n",
    "                if delta < abs(U_prime[s] - self.U[s]):\n",
    "                    delta = abs(U_prime[s] - self.U[s])\n",
    "            if verbose:\n",
    "                print()\n",
    "        if verbose:\n",
    "            print(delta)\n",
    "        if delta < error * (1-discount) / discount:\n",
    "            break\n",
    "            \n",
    "ThreeByThreeWorld.value_iteration = value_iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. *r* = 100\n",
    "* The agent tries to stay at (1,3) so the policy at that state will be 'u' or 'l'.\n",
    "* States nearby the terminal state tend to show avoiding policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r = 100\n",
      "l l . \n",
      "u l d \n",
      "u l l \n"
     ]
    }
   ],
   "source": [
    "r = 100\n",
    "print('r = {}'.format(r))\n",
    "world = ThreeByThreeWorld(r)\n",
    "world.value_iteration(0.99, 0.01)\n",
    "pi = world.get_pi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. *r* = -3\n",
    "* The agent tries to reach the goal state (3,3) also avoiding (1,3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r = -3\n",
      "r r . \n",
      "r r u \n",
      "r r u \n"
     ]
    }
   ],
   "source": [
    "r = -3\n",
    "print('r = {}'.format(r))\n",
    "world = ThreeByThreeWorld(r)\n",
    "world.value_iteration(0.99, 0.01)\n",
    "pi = world.get_pi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. *r* = 0\n",
    "* The agent tries to reach the goal state (3,3) also chasing (1,3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r = 0\n",
      "r r . \n",
      "u u u \n",
      "u u u \n"
     ]
    }
   ],
   "source": [
    "r = 0\n",
    "print('r = {}'.format(r))\n",
    "world = ThreeByThreeWorld(r)\n",
    "world.value_iteration(0.99, 0.01)\n",
    "pi = world.get_pi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. *r* = +3\n",
    "* The agent tries stay at state (1,3) and get reward r=3 iteratively since it is the better profit than reaching the goal state (3,3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r = 3\n",
      "l l . \n",
      "u l d \n",
      "u l l \n"
     ]
    }
   ],
   "source": [
    "r = 3\n",
    "print('r = {}'.format(r))\n",
    "world = ThreeByThreeWorld(r)\n",
    "world.value_iteration(0.99, 0.01)\n",
    "pi = world.get_pi()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
